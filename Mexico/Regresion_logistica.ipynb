{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Cargamos librerias\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import scipy.special as special\n",
    "import scipy.optimize as curve_fit\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import r2_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 26582 entries, 0 to 26581\n",
      "Data columns (total 50 columns):\n",
      " #   Column                       Non-Null Count  Dtype  \n",
      "---  ------                       --------------  -----  \n",
      " 0   host_name                    26582 non-null  object \n",
      " 1   host_since                   26582 non-null  object \n",
      " 2   host_location                26582 non-null  object \n",
      " 3   host_response_time           26582 non-null  object \n",
      " 4   host_response_rate           26582 non-null  object \n",
      " 5   host_is_superhost            26582 non-null  object \n",
      " 6   host_neighbourhood           26582 non-null  object \n",
      " 7   host_verifications           26582 non-null  object \n",
      " 8   host_has_profile_pic         26582 non-null  object \n",
      " 9   host_identity_verified       26582 non-null  object \n",
      " 10  neighbourhood                26582 non-null  object \n",
      " 11  neighbourhood_cleansed       26582 non-null  object \n",
      " 12  property_type                26582 non-null  object \n",
      " 13  room_type                    26582 non-null  object \n",
      " 14  bathrooms_text               26582 non-null  object \n",
      " 15  amenities                    26582 non-null  object \n",
      " 16  has_availability             26582 non-null  object \n",
      " 17  first_review                 26582 non-null  object \n",
      " 18  last_review                  26582 non-null  object \n",
      " 19  license                      26582 non-null  object \n",
      " 20  instant_bookable             26582 non-null  object \n",
      " 21  host_id                      26582 non-null  int64  \n",
      " 22  host_acceptance_rate         26582 non-null  float64\n",
      " 23  host_total_listings_count    26582 non-null  float64\n",
      " 24  latitude                     26582 non-null  float64\n",
      " 25  longitude                    26582 non-null  float64\n",
      " 26  accommodates                 26582 non-null  float64\n",
      " 27  bathrooms                    26582 non-null  float64\n",
      " 28  bedrooms                     26582 non-null  float64\n",
      " 29  beds                         26582 non-null  float64\n",
      " 30  price                        26582 non-null  float64\n",
      " 31  minimum_nights               26582 non-null  float64\n",
      " 32  maximum_nights               26582 non-null  int64  \n",
      " 33  minimum_nights_avg_ntm       26582 non-null  float64\n",
      " 34  maximum_nights_avg_ntm       26582 non-null  float64\n",
      " 35  availability_30              26582 non-null  int64  \n",
      " 36  availability_60              26582 non-null  int64  \n",
      " 37  availability_90              26582 non-null  int64  \n",
      " 38  availability_365             26582 non-null  int64  \n",
      " 39  number_of_reviews            26582 non-null  float64\n",
      " 40  number_of_reviews_ltm        26582 non-null  float64\n",
      " 41  number_of_reviews_l30d       26582 non-null  float64\n",
      " 42  review_scores_rating         26582 non-null  float64\n",
      " 43  review_scores_accuracy       26582 non-null  float64\n",
      " 44  review_scores_cleanliness    26582 non-null  float64\n",
      " 45  review_scores_checkin        26582 non-null  float64\n",
      " 46  review_scores_communication  26582 non-null  float64\n",
      " 47  review_scores_location       26582 non-null  float64\n",
      " 48  review_scores_value          26582 non-null  float64\n",
      " 49  reviews_per_month            26582 non-null  float64\n",
      "dtypes: float64(23), int64(6), object(21)\n",
      "memory usage: 10.1+ MB\n"
     ]
    }
   ],
   "source": [
    "#Cargar archivo csv\n",
    "df = pd.read_csv('50_sin_nulos_ni_atipicos_MexicoCity_DistritoFederal_Mexico.csv')\n",
    "df = df.drop(['Unnamed: 0'], axis = 1)\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Remplazar valores**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Unknown' 'f' 't']\n",
      "['Unknown' 'f' 't']\n",
      "['f' 't']\n",
      "['Boat' 'Campsite' 'Casa particular' 'Castle' 'Dome' 'Earthen home'\n",
      " 'Entire bungalow' 'Entire cabin' 'Entire chalet' 'Entire condo'\n",
      " 'Entire cottage' 'Entire guest suite' 'Entire guesthouse' 'Entire home'\n",
      " 'Entire home/apt' 'Entire hostel' 'Entire in-law' 'Entire loft'\n",
      " 'Entire place' 'Entire rental unit' 'Entire serviced apartment'\n",
      " 'Entire townhouse' 'Entire vacation home' 'Entire villa' 'Farm stay'\n",
      " 'Holiday park' 'Hut' 'Private room' 'Private room in barn'\n",
      " 'Private room in bed and breakfast' 'Private room in bungalow'\n",
      " 'Private room in cabin' 'Private room in casa particular'\n",
      " 'Private room in castle' 'Private room in chalet' 'Private room in condo'\n",
      " 'Private room in cottage' 'Private room in dome' 'Private room in dorm'\n",
      " 'Private room in earthen home' 'Private room in farm stay'\n",
      " 'Private room in floor' 'Private room in guest suite'\n",
      " 'Private room in guesthouse' 'Private room in home'\n",
      " 'Private room in hostel' 'Private room in houseboat'\n",
      " 'Private room in hut' 'Private room in lighthouse' 'Private room in loft'\n",
      " 'Private room in minsu' 'Private room in nature lodge'\n",
      " 'Private room in pension' 'Private room in rental unit'\n",
      " 'Private room in resort' 'Private room in serviced apartment'\n",
      " 'Private room in shipping container' 'Private room in tent'\n",
      " 'Private room in tiny home' 'Private room in tower'\n",
      " 'Private room in townhouse' 'Private room in vacation home'\n",
      " 'Private room in villa' 'Room in aparthotel' 'Room in bed and breakfast'\n",
      " 'Room in boutique hotel' 'Room in casa particular' 'Room in hostel'\n",
      " 'Room in hotel' 'Room in serviced apartment'\n",
      " 'Shared room in bed and breakfast' 'Shared room in boutique hotel'\n",
      " 'Shared room in cabin' 'Shared room in casa particular'\n",
      " 'Shared room in condo' 'Shared room in dorm' 'Shared room in farm stay'\n",
      " 'Shared room in guest suite' 'Shared room in guesthouse'\n",
      " 'Shared room in home' 'Shared room in hostel' 'Shared room in hotel'\n",
      " 'Shared room in loft' 'Shared room in rental unit'\n",
      " 'Shared room in serviced apartment' 'Shared room in tent'\n",
      " 'Shared room in tiny home' 'Shared room in townhouse'\n",
      " 'Shipping container' 'Tent' 'Tiny home' 'Tower']\n",
      "['Entire home/apt' 'Hotel room' 'Private room' 'Shared room']\n",
      "['f' 't']\n",
      "['A Coruña, Spain' 'Abuja, Nigeria' 'Acapulco de Juárez, Mexico'\n",
      " 'Acapulco, Mexico' 'Acopilco, Mexico' 'Aguascalientes, Mexico'\n",
      " 'Airdrie, Canada' 'Ajijic, Mexico' 'Albuquerque, NM' 'Alesso, Italy'\n",
      " 'Alfredo V. Bonfil, Mexico' 'Alicante, Spain' 'Aliso Viejo, CA'\n",
      " 'Amatlán de Quetzalcóatl, Mexico' 'Amsterdam, Netherlands'\n",
      " 'Antwerpen, Belgium' 'Aspen, CO' 'Auckland, New Zealand' 'Austin, TX'\n",
      " 'Bacalar, Mexico' 'Bakersfield, CA' 'Barcelona, Spain' 'Basalt, CO'\n",
      " 'Bat Yam, Israel' 'Beaver Dam, WI' 'Bergen, Norway' 'Berkeley, CA'\n",
      " 'Berlin, Germany' 'Bern, Switzerland' 'Bilbao, Spain' 'Bogota, Colombia'\n",
      " 'Bogotá, Colombia' 'Bonn, Germany' 'Bora-Bora, French Polynesia'\n",
      " 'Boston, MA' 'Brussels, Belgium' 'Bucharest, Romania'\n",
      " 'Buenos Aires, Argentina' 'Burford, United Kingdom'\n",
      " 'Burton upon Trent, United Kingdom' 'Cabo San Lucas, Mexico'\n",
      " 'Cacalomacán, Mexico' 'Cairo, Egypt' 'Calgary, Canada' 'Cali, Colombia'\n",
      " 'California, United States' 'Cambridge, MA' 'Campeche, Mexico' 'Canada'\n",
      " 'Canberra, Australia' 'Cancún, Mexico' 'Capital District, Venezuela'\n",
      " 'Caracas, Venezuela' 'Cartagena, Colombia' 'Cedar Park, TX'\n",
      " 'Chetumal, Mexico' 'Chiapas, Mexico' 'Chicago, IL' 'Chihuahua, Mexico'\n",
      " 'Chilpancingo, Mexico' 'Cholul, Mexico' 'Cholula, Mexico'\n",
      " 'Churubusco, Mexico' 'Ciudad Cuauhtemoc, Mexico' 'Ciudad Juarez, Mexico'\n",
      " 'Ciudad Juárez, Mexico' 'Ciudad López Mateos, Mexico'\n",
      " 'Ciudad Nezahualcóyotl, Mexico' 'Ciudad Victoria, Mexico'\n",
      " 'Ciudad de Mexico, Mexico' 'Ciudad de México, Mexico'\n",
      " 'Ciudad del Carmen, Mexico' 'Clovis, CA' 'Coacalco, Mexico'\n",
      " 'Coatzacoalcos, Mexico' 'Cocoyoc, Morelos, Mexico' 'Colima, Mexico'\n",
      " 'Cologne, Germany' 'Colombia' 'Colonia Juárez, Mexico'\n",
      " 'Concepción, Chile' 'Copenhagen, Denmark' 'Coronado, CA'\n",
      " 'Corpus Christi, TX' 'Cozumel, Mexico' 'Cuajimalpa, Mexico'\n",
      " 'Cuauhtémoc, Mexico' 'Cuautitlán Izcalli, Mexico' 'Cuernavaca, Mexico'\n",
      " 'Culiacán Rosales, Mexico' 'Culiacán, Mexico' 'Cumpas, Mexico'\n",
      " 'Cypress, TX' 'Córdoba, Argentina' 'Córdoba, Spain' 'Dallas, TX'\n",
      " 'Denver, CO' 'Dubai, United Arab Emirates' 'Dublin, Ireland'\n",
      " 'Durango, Mexico' 'Durham, NC' 'Eagle Pass, TX'\n",
      " 'Ecatepec de Morelos, Mexico' 'Edinburg, TX' 'Edmonton, Canada'\n",
      " 'Ejidos de San Andrés Totoltepec, Mexico' 'El Paso, TX'\n",
      " 'El Pueblito, Mexico' 'El Real, Mexico' 'El Tigre, Mexico'\n",
      " 'Emiliano Zapata, Mexico' 'England, United Kingdom' 'Englewood, NJ'\n",
      " 'Ensenada, Mexico' 'Estado de México, Mexico' 'Etzatlán, Mexico'\n",
      " 'Falmouth, MA' 'Federal District, Mexico' 'Florence, Italy'\n",
      " 'Florida, Argentina' 'Florida, United States' 'Fort Lauderdale, FL'\n",
      " 'Framingham, MA' 'France' 'Frankfurt, Germany' 'Gardena, CA'\n",
      " 'Geneva, Switzerland' 'Georgia, United States' 'Gijón, Spain'\n",
      " 'Guadalajara, Mexico' 'Guanajuato, Mexico' 'Guatemala City, Guatemala'\n",
      " 'Guayaquil, Ecuador' 'Guerrero, Mexico' 'Hallandale Beach, FL'\n",
      " 'Hazel Park, MI' 'Heredia Province, Costa Rica' 'Hermosillo, Mexico'\n",
      " 'Heroica Veracruz, Mexico' 'Hidalgo, Mexico' 'Hollywood, FL' 'Hong Kong'\n",
      " 'Houston, TX' 'Huauchinango, Mexico' 'Huichapan, Mexico'\n",
      " 'Huitzilac, Mexico' 'Huixquilucan de Degollado, Mexico'\n",
      " 'Huixquilucan, Mexico' 'Irapuato, Mexico' 'Irvine, CA'\n",
      " 'Ixtapaluca, Mexico' 'Jalisco, Mexico' 'Jaltenco, Mexico'\n",
      " 'Jiutepec, Mexico' 'Juriquilla, Mexico' 'Katy, TX' 'Kelowna, Canada'\n",
      " 'Kuala Lumpur, Malaysia' 'Kuna, ID' 'La Crucecita, Mexico'\n",
      " 'La Garenne-Colombes, France' 'La Paz, Mexico' 'La Réunion, France'\n",
      " 'Las Palmas de Gran Canaria, Spain' 'Las Vegas, NV'\n",
      " 'Lausanne, Switzerland' 'Lawrence, KS' 'Leeds, United Kingdom'\n",
      " 'Leon, Mexico' 'Lerma de Villada, Mexico' 'León, Mexico' 'Lomma, Sweden'\n",
      " 'London Borough of Lambeth, United Kingdom' 'London, United Kingdom'\n",
      " 'Los Angeles, CA' 'Los Mochis, Mexico' 'Līvāni Municipality, Latvia'\n",
      " 'Madrid, Spain' 'Mahahual, Mexico' 'Malaysia' 'Manzanillo, Mexico'\n",
      " 'Marseille, France' 'Matamoros, Mexico' 'Mazatlán, Mexico' 'McAllen, TX'\n",
      " 'Mendoza, Argentina' 'Merida, Mexico' 'Merion Station, PA'\n",
      " 'Metepec, Mexico' 'Mexicali, Mexico' 'Mexico' 'Mexico City, Mexico'\n",
      " 'Mexico, NY' 'Miami Beach, FL' 'Miami, FL' 'Milan, Italy'\n",
      " 'Mill Valley, CA' 'Milwaukee, WI' 'Minneapolis, MN' 'Mississauga, Canada'\n",
      " 'Missoula, MT' 'Mogilev, Belarus' 'Mont-Saint-Aignan, France'\n",
      " 'Monterey Park, CA' 'Monterrey, Mexico' 'Montevideo, Uruguay'\n",
      " 'Montpellier, France' 'Montreal, Canada' 'Morelia, Mexico'\n",
      " 'Morelos, Mexico' 'Mérida, Mexico' 'Nacajuca, Mexico' 'Naples, FL'\n",
      " 'Nashville, TN' 'Naucalpan de Juárez, Mexico' 'Naucalpan, Mexico'\n",
      " 'Nayarit, Mexico' 'New Haven, CT' 'New Jersey, United States'\n",
      " 'New Orleans, LA' 'New York, NY' 'New York, United States' 'Nicaragua'\n",
      " 'Nice, France' 'No location' 'Nuevo Casas Grandes, Mexico'\n",
      " 'Nuevo Vallarta, Mexico' 'Oakland, CA' 'Oaxaca de Juárez, Mexico'\n",
      " 'Oaxaca, Mexico' 'Ojo de Agua, Mexico' 'Oklahoma, United States'\n",
      " 'Orizaba, Mexico' 'Orlando, FL' 'Ostuni, Italy'\n",
      " 'Ozumba de Alzate, Mexico' 'Pachuca, Mexico' 'Palenque, Mexico'\n",
      " 'Palermo, Italy' 'Palma, Spain' 'Pamplona, Spain' 'Panama'\n",
      " 'Paraíso, Mexico' 'Paris, France' 'Parral, Mexico'\n",
      " 'Parras de la Fuente, Mexico' 'Pau, France' 'Pennsylvania, United States'\n",
      " 'Peru' 'Philadelphia, PA' 'Phoenix, AZ' 'Phoenixville, PA'\n",
      " 'Pittsburgh, PA' 'Playa del Carmen, Mexico' 'Pombal, Portugal'\n",
      " 'Portland, OR' 'Porto, Portugal' 'Pownal, VT' 'Prague, Czechia'\n",
      " 'Puebla, Mexico' 'Puerto Aventuras, Mexico' 'Puerto Escondido, Mexico'\n",
      " 'Puerto Rico' 'Puerto Vallarta, Mexico' 'Pátzcuaro, Mexico'\n",
      " 'Querétaro, Mexico' 'Quintana Roo, Mexico' 'Quito, Ecuador'\n",
      " 'Ramos Mejía, Argentina' 'Renton, WA' 'Richardson, TX' 'Rocklin, CA'\n",
      " 'Rome, Italy' 'Rosarito, Mexico' 'Rotterdam, Netherlands'\n",
      " 'Sahagun City, Mexico' 'Saltillo, Mexico' 'Salvador, Brazil'\n",
      " 'San Andres Cholula, Mexico' 'San Andrés Cholula, Mexico'\n",
      " 'San Andrés Mixquic, Mexico' 'San Antonio Tecómitl, Mexico'\n",
      " 'San Antonio, Chile' 'San Antonio, TX'\n",
      " 'San Cristobal de las Casas, Mexico' 'San Cristóbal de Las Casas, Mexico'\n",
      " 'San Diego, CA' 'San Francisco, CA' 'San Jerónimo Ejido, Mexico'\n",
      " 'San Jose, CA' 'San José de los Olvera, Mexico'\n",
      " 'San José del Cabo, Mexico' 'San Juan Cuautlancingo, Mexico'\n",
      " 'San Juan Teotihuacán, Mexico' 'San Luis Potosi, Mexico'\n",
      " 'San Miguel Ajusco, Mexico' 'San Miguel de Allende, Mexico'\n",
      " 'San Miguel de Tucumán, Argentina' 'San Miguel, Mexico'\n",
      " 'San Pedro Apatlaco, Mexico' 'San Pedro Atocpan, Mexico'\n",
      " 'San Pedro Cholula, Mexico' 'San Pedro Garza García, Mexico'\n",
      " 'San Salvador Tizatlalli, Mexico' 'Sanctorum, Mexico' 'Santa Barbara, CA'\n",
      " 'Santa Clara del Cobre, Mexico' 'Santa Clarita, CA' 'Santa Cruz, CA'\n",
      " 'Santa María Atzompa, Mexico' 'Santa Monica, CA'\n",
      " 'Santa Rosa Jáuregui, Mexico' 'Santa Rosa, CA'\n",
      " 'Santiago de Querétaro, Mexico' 'Santiago, Chile' 'Sayulita, Mexico'\n",
      " 'Seattle, WA' 'Sharjah, United Arab Emirates' 'Shibuya, Japan'\n",
      " 'Singapore' 'Sonora, Mexico' 'Spain' 'Spring, TX' 'Springdale, AR'\n",
      " 'Springfield, VA' 'State of Mexico, Mexico' 'Stockholm, Sweden'\n",
      " 'Sunny Isles Beach, FL' 'Sunnyvale, CA' 'Surfside, FL' 'Switzerland'\n",
      " 'Sydney, Australia' 'São Paulo, Brazil' 'Tampico, Mexico'\n",
      " 'Taoyuan City, Taiwan' 'Tapachula de Córdova y Ordoñez, Mexico'\n",
      " 'Tapachula, Mexico' 'Tarragona, Spain' 'Taxco, Mexico'\n",
      " 'Tegucigalpa, Honduras' 'Temixco, Mexico' 'Tepexpan, Mexico'\n",
      " 'Tepotzotlán, Mexico' 'Tepoztlán, Mexico' 'Tequisquiapan, Mexico'\n",
      " 'Texcoco, Mexico' 'Tezoyuca, Mexico' 'Tijuana, Mexico' 'Tizayuca, Mexico'\n",
      " 'Tlalnepantla de Baz, Mexico' 'Tlalnepantla, Mexico' 'Tlaxcala, Mexico'\n",
      " 'Tláhuac, Mexico' 'Toluca, Mexico' 'Topanga, CA' 'Toronto, Canada'\n",
      " 'Torpoint, United Kingdom' 'Torrance, CA' 'Torreón, Mexico'\n",
      " 'Tours, France' 'Tres de Mayo, Mexico' 'Truckee, CA' 'Tula, Mexico'\n",
      " 'Tulum, Mexico' 'Tuxtla Gutiérrez, Mexico' 'United Kingdom'\n",
      " 'United States' 'Unión Hidalgo, Mexico' 'Uruapan, Mexico'\n",
      " 'Valencia, Spain' 'Valladolid, Mexico' 'Valle de Bravo, Mexico'\n",
      " 'Valle de Chalco, Mexico' 'Vancouver, Canada' 'Vannes, France'\n",
      " 'Vicente Guerrero, Mexico' 'Victoria, Canada' 'Viken, Norway'\n",
      " 'Vilcabamba, Ecuador' 'Villa Milpa Alta, Mexico'\n",
      " 'Villa de Álvarez, Mexico' 'Villahermosa, Mexico' 'Virum, Denmark'\n",
      " 'Walnut Creek, CA' 'Washington, D.C., DC' 'Washington, DC'\n",
      " 'Werneck, Germany' 'Weston, FL' 'Whistler, Canada' 'Windermere, Canada'\n",
      " 'Winnipeg, Canada' 'Winston-Salem, NC' 'Xalapa, Mexico'\n",
      " 'Xicotepec de Juárez, Mexico' 'Xochimilco, Mexico' 'Yecapixtla, Mexico'\n",
      " 'Zamora, Mexico' 'Zapopan, Mexico' 'Zurich, Switzerland'\n",
      " 'Zürich, Switzerland' 'Álvaro Obregón, Mexico']\n",
      "['Not defined' 'a few days or more' 'within a day' 'within a few hours'\n",
      " 'within an hour']\n",
      "[0.  1.  1.4 2.  3. ]\n",
      "['f' 't']\n"
     ]
    }
   ],
   "source": [
    "#Verificar los valores sin repertirse de una columna\n",
    "unico1 = np.unique(df['host_is_superhost'])\n",
    "unico2 = np.unique(df['has_availability'])\n",
    "unico3 = np.unique(df['instant_bookable'])\n",
    "unico4 = np.unique(df['property_type'])\n",
    "unico5 = np.unique(df['room_type'])\n",
    "unico6 = np.unique(df['host_identity_verified'])\n",
    "unico7 = np.unique(df['host_location'])\n",
    "unico8 = np.unique(df['host_response_time'])\n",
    "unico9 = np.unique(df['bedrooms'])\n",
    "unico10 = np.unique(df['host_has_profile_pic'])\n",
    "print(unico1)\n",
    "print(unico2)\n",
    "print(unico3)\n",
    "print(unico4)\n",
    "print(unico5)\n",
    "print(unico6)\n",
    "print(unico7)\n",
    "print(unico8)\n",
    "print(unico9)\n",
    "print(unico10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Convierto  una variable a dicotomica\n",
    "df['host_is_superhost'] =df['host_is_superhost'].replace([\"Unknown\"], \"f\")\n",
    "df['has_availability'] =df['has_availability'].replace([\"Unknown\"], \"f\")\n",
    "#df['instant_bookable'] =df['instant_bookable'].replace([\"Unknown\"], \"f\")\n",
    "df['property_type'] = df['property_type'].mask(df['property_type'] != \"Entire rental unit\", \"Other property\")\n",
    "df['room_type'] =df['room_type'].replace([\"Hotel room\", \"Private room\", \"Shared room\"], \"Other room\")\n",
    "#df['host_identity_verified'] =df['host_identity_verified'].replace([\"Unknown\"], \"f\")\n",
    "df['host_location'] =df['host_location'].mask(df['host_location'] != \"Ciudad de Mexico, Mexico\", \"Other location\")\n",
    "df['host_response_time'] =df['host_response_time'].replace([\"Not defined\", \"a few days or more\", \"within a day\", \"within a few hours\"], \"More than one hour\")\n",
    "df['bedrooms'] =df['bedrooms'].replace([1, 1.5, 3, 1.4], 0)\n",
    "#df['host_has_profile_pic'] =df['host_has_profile_pic'].replace([\"Unknown\"], \"f\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Corroboramos valores nulos\n",
    "valores_nulos = df.isnull().sum().sum()\n",
    "valores_nulos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Prediccion host_is_superhost**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Matriz de Confusion: \n",
      "[[5069    0]\n",
      " [2906    0]]\n",
      "Precision del modelo:  0.0\n",
      "Exactitud del modelo:  0.6356112852664577\n",
      "Sensibilidad del modelo:  0.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\lmanu\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    }
   ],
   "source": [
    "#Declaramos las variables dependientes e idependientes para la regresion logistica\n",
    "Vars_Indep = df[['review_scores_checkin', 'review_scores_cleanliness', 'review_scores_communication']] #Numericas\n",
    "Var_Dep = df['host_is_superhost'] #Dicotomica\n",
    "#Redefinimos las variables\n",
    "X = Vars_Indep\n",
    "y = Var_Dep\n",
    "#Dividimos el conjunto de datos en la parte de entrenamiento y prueba:\n",
    "X_train, X_test, y_train, y_test =train_test_split(X, y, test_size= 0.3, random_state=None)\n",
    "#Se escalan todos los datos\n",
    "escalar = StandardScaler()\n",
    "#Para realizar el escalamiento de las variables \"X\" tanto de entrenamiento como de prueba, utilizaremos fit_transform\n",
    "X_train = escalar.fit_transform(X_train)\n",
    "X_test = escalar.transform(X_test)\n",
    "#Definimos el algoritmo a utilizar\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "algoritmo = LogisticRegression()\n",
    "#Entrenamos al modelo\n",
    "algoritmo.fit(X_train, y_train)\n",
    "#Realizamos una prediccion\n",
    "y_pred = algoritmo.predict(X_test)\n",
    "#Verifico la matriz de confusion\n",
    "from sklearn.metrics import confusion_matrix\n",
    "matriz = confusion_matrix(y_test, y_pred)\n",
    "print('Matriz de Confusion: ')\n",
    "print(matriz)\n",
    "#Calculo la precision del modelo\n",
    "from sklearn.metrics import precision_score\n",
    "precision = precision_score(y_test, y_pred, average=\"binary\", pos_label=\"t\")\n",
    "print('Precision del modelo: ', precision)\n",
    "#Calculamos la exactitud del modelo\n",
    "from sklearn.metrics import accuracy_score\n",
    "exactitud = accuracy_score(y_test, y_pred)\n",
    "print('Exactitud del modelo: ', exactitud)\n",
    "#Calculo la sensibilidad del modelo\n",
    "from sklearn.metrics import recall_score\n",
    "sensibilidad = recall_score(y_test, y_pred, average=\"binary\", pos_label=\"t\")\n",
    "print(\"Sensibilidad del modelo: \", sensibilidad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Prediccion has_availability**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Matriz de Confusion: \n",
      "[[   0  324]\n",
      " [   0 7651]]\n",
      "Precision del modelo:  0.9593730407523511\n",
      "Exactitud del modelo:  0.9593730407523511\n",
      "Sensibilidad del modelo:  1.0\n"
     ]
    }
   ],
   "source": [
    "#Declaramos las variables dependientes e idependientes para la regresion logistica\n",
    "Vars_Indep = df[['availability_30', 'availability_60', 'availability_90']] #Numericas\n",
    "Var_Dep = df['has_availability'] #Dicotomica\n",
    "#Redefinimos las variables\n",
    "X = Vars_Indep\n",
    "y = Var_Dep\n",
    "#Dividimos el conjunto de datos en la parte de entrenamiento y prueba:\n",
    "X_train, X_test, y_train, y_test =train_test_split(X, y, test_size= 0.3, random_state=None)\n",
    "#Se escalan todos los datos\n",
    "escalar = StandardScaler()\n",
    "#Para realizar el escalamiento de las variables \"X\" tanto de entrenamiento como de prueba, utilizaremos fit_transform\n",
    "X_train = escalar.fit_transform(X_train)\n",
    "X_test = escalar.transform(X_test)\n",
    "#Definimos el algoritmo a utilizar\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "algoritmo = LogisticRegression()\n",
    "#Entrenamos al modelo\n",
    "algoritmo.fit(X_train, y_train)\n",
    "#Realizamos una prediccion\n",
    "y_pred = algoritmo.predict(X_test)\n",
    "#Verifico la matriz de confusion\n",
    "from sklearn.metrics import confusion_matrix\n",
    "matriz = confusion_matrix(y_test, y_pred)\n",
    "print('Matriz de Confusion: ')\n",
    "print(matriz)\n",
    "#Calculo la precision del modelo\n",
    "from sklearn.metrics import precision_score\n",
    "precision = precision_score(y_test, y_pred, average=\"binary\", pos_label=\"t\")\n",
    "print('Precision del modelo: ', precision)\n",
    "#Calculamos la exactitud del modelo\n",
    "from sklearn.metrics import accuracy_score\n",
    "exactitud = accuracy_score(y_test, y_pred)\n",
    "print('Exactitud del modelo: ', exactitud)\n",
    "#Calculo la sensibilidad del modelo\n",
    "from sklearn.metrics import recall_score\n",
    "sensibilidad = recall_score(y_test, y_pred, average=\"binary\", pos_label=\"t\")\n",
    "print(\"Sensibilidad del modelo: \", sensibilidad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Prediccion instant_bookable**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Matriz de Confusion: \n",
      "[[3844 1004]\n",
      " [1473 1654]]\n",
      "Precision del modelo:  0.6222723852520692\n",
      "Exactitud del modelo:  0.6894043887147335\n",
      "Sensibilidad del modelo:  0.5289414774544292\n"
     ]
    }
   ],
   "source": [
    "#Declaramos las variables dependientes e idependientes para la regresion logistica\n",
    "Vars_Indep = df[['host_acceptance_rate', 'price', 'availability_365']] #Numericas\n",
    "Var_Dep = df['instant_bookable'] #Dicotomica\n",
    "#Redefinimos las variables\n",
    "X = Vars_Indep\n",
    "y = Var_Dep\n",
    "#Dividimos el conjunto de datos en la parte de entrenamiento y prueba:\n",
    "X_train, X_test, y_train, y_test =train_test_split(X, y, test_size= 0.3, random_state=None)\n",
    "#Se escalan todos los datos\n",
    "escalar = StandardScaler()\n",
    "#Para realizar el escalamiento de las variables \"X\" tanto de entrenamiento como de prueba, utilizaremos fit_transform\n",
    "X_train = escalar.fit_transform(X_train)\n",
    "X_test = escalar.transform(X_test)\n",
    "#Definimos el algoritmo a utilizar\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "algoritmo = LogisticRegression()\n",
    "#Entrenamos al modelo\n",
    "algoritmo.fit(X_train, y_train)\n",
    "#Realizamos una prediccion\n",
    "y_pred = algoritmo.predict(X_test)\n",
    "#Verifico la matriz de confusion\n",
    "from sklearn.metrics import confusion_matrix\n",
    "matriz = confusion_matrix(y_test, y_pred)\n",
    "print('Matriz de Confusion: ')\n",
    "print(matriz)\n",
    "#Calculo la precision del modelo\n",
    "from sklearn.metrics import precision_score\n",
    "precision = precision_score(y_test, y_pred, average=\"binary\", pos_label=\"t\")\n",
    "print('Precision del modelo: ', precision)\n",
    "#Calculamos la exactitud del modelo\n",
    "from sklearn.metrics import accuracy_score\n",
    "exactitud = accuracy_score(y_test, y_pred)\n",
    "print('Exactitud del modelo: ', exactitud)\n",
    "#Calculo la sensibilidad del modelo\n",
    "from sklearn.metrics import recall_score\n",
    "sensibilidad = recall_score(y_test, y_pred, average=\"binary\", pos_label=\"t\")\n",
    "print(\"Sensibilidad del modelo: \", sensibilidad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Prediccion property_type**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Matriz de Confusion: \n",
      "[[1037 2003]\n",
      " [ 835 4100]]\n",
      "Precision del modelo:  0.5539529914529915\n",
      "Exactitud del modelo:  0.6441379310344828\n",
      "Sensibilidad del modelo:  0.34111842105263157\n"
     ]
    }
   ],
   "source": [
    "#Declaramos las variables dependientes e idependientes para la regresion logistica\n",
    "Vars_Indep = df[['review_scores_location', 'accommodates', 'price']] #Numericas\n",
    "Var_Dep = df['property_type'] #Dicotomica\n",
    "#Redefinimos las variables\n",
    "X = Vars_Indep\n",
    "y = Var_Dep\n",
    "#Dividimos el conjunto de datos en la parte de entrenamiento y prueba:\n",
    "X_train, X_test, y_train, y_test =train_test_split(X, y, test_size= 0.3, random_state=None)\n",
    "#Se escalan todos los datos\n",
    "escalar = StandardScaler()\n",
    "#Para realizar el escalamiento de las variables \"X\" tanto de entrenamiento como de prueba, utilizaremos fit_transform\n",
    "X_train = escalar.fit_transform(X_train)\n",
    "X_test = escalar.transform(X_test)\n",
    "#Definimos el algoritmo a utilizar\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "algoritmo = LogisticRegression()\n",
    "#Entrenamos al modelo\n",
    "algoritmo.fit(X_train, y_train)\n",
    "#Realizamos una prediccion\n",
    "y_pred = algoritmo.predict(X_test)\n",
    "#Verifico la matriz de confusion\n",
    "from sklearn.metrics import confusion_matrix\n",
    "matriz = confusion_matrix(y_test, y_pred)\n",
    "print('Matriz de Confusion: ')\n",
    "print(matriz)\n",
    "#Calculo la precision del modelo\n",
    "from sklearn.metrics import precision_score\n",
    "precision = precision_score(y_test, y_pred, average=\"binary\", pos_label=\"Entire rental unit\")\n",
    "print('Precision del modelo: ', precision)\n",
    "#Calculamos la exactitud del modelo\n",
    "from sklearn.metrics import accuracy_score\n",
    "exactitud = accuracy_score(y_test, y_pred)\n",
    "print('Exactitud del modelo: ', exactitud)\n",
    "#Calculo la sensibilidad del modelo\n",
    "from sklearn.metrics import recall_score\n",
    "sensibilidad = recall_score(y_test, y_pred, average=\"binary\", pos_label=\"Entire rental unit\")\n",
    "print(\"Sensibilidad del modelo: \", sensibilidad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Prediccion room_type**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Matriz de Confusion: \n",
      "[[4514  686]\n",
      " [ 874 1901]]\n",
      "Precision del modelo:  0.8377876763177431\n",
      "Exactitud del modelo:  0.8043887147335423\n",
      "Sensibilidad del modelo:  0.8680769230769231\n"
     ]
    }
   ],
   "source": [
    "#Declaramos las variables dependientes e idependientes para la regresion logistica\n",
    "Vars_Indep = df[['price', 'accommodates', 'bathrooms']] #Numericas\n",
    "Var_Dep = df['room_type'] #Dicotomica\n",
    "#Redefinimos las variables\n",
    "X = Vars_Indep\n",
    "y = Var_Dep\n",
    "#Dividimos el conjunto de datos en la parte de entrenamiento y prueba:\n",
    "X_train, X_test, y_train, y_test =train_test_split(X, y, test_size= 0.3, random_state=None)\n",
    "#Se escalan todos los datos\n",
    "escalar = StandardScaler()\n",
    "#Para realizar el escalamiento de las variables \"X\" tanto de entrenamiento como de prueba, utilizaremos fit_transform\n",
    "X_train = escalar.fit_transform(X_train)\n",
    "X_test = escalar.transform(X_test)\n",
    "#Definimos el algoritmo a utilizar\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "algoritmo = LogisticRegression()\n",
    "#Entrenamos al modelo\n",
    "algoritmo.fit(X_train, y_train)\n",
    "#Realizamos una prediccion\n",
    "y_pred = algoritmo.predict(X_test)\n",
    "#Verifico la matriz de confusion\n",
    "from sklearn.metrics import confusion_matrix\n",
    "matriz = confusion_matrix(y_test, y_pred)\n",
    "print('Matriz de Confusion: ')\n",
    "print(matriz)\n",
    "#Calculo la precision del modelo\n",
    "from sklearn.metrics import precision_score\n",
    "precision = precision_score(y_test, y_pred, average=\"binary\", pos_label=\"Entire home/apt\")\n",
    "print('Precision del modelo: ', precision)\n",
    "#Calculamos la exactitud del modelo\n",
    "from sklearn.metrics import accuracy_score\n",
    "exactitud = accuracy_score(y_test, y_pred)\n",
    "print('Exactitud del modelo: ', exactitud)\n",
    "#Calculo la sensibilidad del modelo\n",
    "from sklearn.metrics import recall_score\n",
    "sensibilidad = recall_score(y_test, y_pred, average=\"binary\", pos_label=\"Entire home/apt\")\n",
    "print(\"Sensibilidad del modelo: \", sensibilidad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Prediccion host_identity_verified**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Matriz de Confusion: \n",
      "[[   0  348]\n",
      " [   0 7627]]\n",
      "Precision del modelo:  0.9563636363636364\n",
      "Exactitud del modelo:  0.9563636363636364\n",
      "Sensibilidad del modelo:  1.0\n"
     ]
    }
   ],
   "source": [
    "#Declaramos las variables dependientes e idependientes para la regresion logistica\n",
    "Vars_Indep = df[['review_scores_value', 'host_total_listings_count', 'number_of_reviews']] #Numericas\n",
    "Var_Dep = df['host_identity_verified'] #Dicotomica\n",
    "#Redefinimos las variables\n",
    "X = Vars_Indep\n",
    "y = Var_Dep\n",
    "#Dividimos el conjunto de datos en la parte de entrenamiento y prueba:\n",
    "X_train, X_test, y_train, y_test =train_test_split(X, y, test_size= 0.3, random_state=None)\n",
    "#Se escalan todos los datos\n",
    "escalar = StandardScaler()\n",
    "#Para realizar el escalamiento de las variables \"X\" tanto de entrenamiento como de prueba, utilizaremos fit_transform\n",
    "X_train = escalar.fit_transform(X_train)\n",
    "X_test = escalar.transform(X_test)\n",
    "#Definimos el algoritmo a utilizar\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "algoritmo = LogisticRegression()\n",
    "#Entrenamos al modelo\n",
    "algoritmo.fit(X_train, y_train)\n",
    "#Realizamos una prediccion\n",
    "y_pred = algoritmo.predict(X_test)\n",
    "#Verifico la matriz de confusion\n",
    "from sklearn.metrics import confusion_matrix\n",
    "matriz = confusion_matrix(y_test, y_pred)\n",
    "print('Matriz de Confusion: ')\n",
    "print(matriz)\n",
    "#Calculo la precision del modelo\n",
    "from sklearn.metrics import precision_score\n",
    "precision = precision_score(y_test, y_pred, average=\"binary\", pos_label=\"t\")\n",
    "print('Precision del modelo: ', precision)\n",
    "#Calculamos la exactitud del modelo\n",
    "from sklearn.metrics import accuracy_score\n",
    "exactitud = accuracy_score(y_test, y_pred)\n",
    "print('Exactitud del modelo: ', exactitud)\n",
    "#Calculo la sensibilidad del modelo\n",
    "from sklearn.metrics import recall_score\n",
    "sensibilidad = recall_score(y_test, y_pred, average=\"binary\", pos_label=\"t\")\n",
    "print(\"Sensibilidad del modelo: \", sensibilidad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Prediccion host_location**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Matriz de Confusion: \n",
      "[[   0   64]\n",
      " [   0 7911]]\n",
      "Precision del modelo:  0.0\n",
      "Exactitud del modelo:  0.991974921630094\n",
      "Sensibilidad del modelo:  0.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\lmanu\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    }
   ],
   "source": [
    "#Declaramos las variables dependientes e idependientes para la regresion logistica\n",
    "Vars_Indep = df[['review_scores_communication', 'review_scores_location', 'review_scores_cleanliness']] #Numericas\n",
    "Var_Dep = df['host_location'] #Dicotomica\n",
    "#Redefinimos las variables\n",
    "X = Vars_Indep\n",
    "y = Var_Dep\n",
    "#Dividimos el conjunto de datos en la parte de entrenamiento y prueba:\n",
    "X_train, X_test, y_train, y_test =train_test_split(X, y, test_size= 0.3, random_state=None)\n",
    "#Se escalan todos los datos\n",
    "escalar = StandardScaler()\n",
    "#Para realizar el escalamiento de las variables \"X\" tanto de entrenamiento como de prueba, utilizaremos fit_transform\n",
    "X_train = escalar.fit_transform(X_train)\n",
    "X_test = escalar.transform(X_test)\n",
    "#Definimos el algoritmo a utilizar\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "algoritmo = LogisticRegression()\n",
    "#Entrenamos al modelo\n",
    "algoritmo.fit(X_train, y_train)\n",
    "#Realizamos una prediccion\n",
    "y_pred = algoritmo.predict(X_test)\n",
    "#Verifico la matriz de confusion\n",
    "from sklearn.metrics import confusion_matrix\n",
    "matriz = confusion_matrix(y_test, y_pred)\n",
    "print('Matriz de Confusion: ')\n",
    "print(matriz)\n",
    "#Calculo la precision del modelo\n",
    "from sklearn.metrics import precision_score\n",
    "precision = precision_score(y_test, y_pred, average=\"binary\", pos_label=\"Ciudad de Mexico, Mexico\")\n",
    "print('Precision del modelo: ', precision)\n",
    "#Calculamos la exactitud del modelo\n",
    "from sklearn.metrics import accuracy_score\n",
    "exactitud = accuracy_score(y_test, y_pred)\n",
    "print('Exactitud del modelo: ', exactitud)\n",
    "#Calculo la sensibilidad del modelo\n",
    "from sklearn.metrics import recall_score\n",
    "sensibilidad = recall_score(y_test, y_pred, average=\"binary\", pos_label=\"Ciudad de Mexico, Mexico\")\n",
    "print(\"Sensibilidad del modelo: \", sensibilidad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Prediccion host_response_time**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Matriz de Confusion: \n",
      "[[   0 2310]\n",
      " [   0 5665]]\n",
      "Precision del modelo:  0.7103448275862069\n",
      "Exactitud del modelo:  0.7103448275862069\n",
      "Sensibilidad del modelo:  1.0\n"
     ]
    }
   ],
   "source": [
    "#Declaramos las variables dependientes e idependientes para la regresion logistica\n",
    "Vars_Indep = df[['review_scores_communication', 'review_scores_value', 'price']] #Numericas\n",
    "Var_Dep = df['host_response_time'] #Dicotomica\n",
    "#Redefinimos las variables\n",
    "X = Vars_Indep\n",
    "y = Var_Dep\n",
    "#Dividimos el conjunto de datos en la parte de entrenamiento y prueba:\n",
    "X_train, X_test, y_train, y_test =train_test_split(X, y, test_size= 0.3, random_state=None)\n",
    "#Se escalan todos los datos\n",
    "escalar = StandardScaler()\n",
    "#Para realizar el escalamiento de las variables \"X\" tanto de entrenamiento como de prueba, utilizaremos fit_transform\n",
    "X_train = escalar.fit_transform(X_train)\n",
    "X_test = escalar.transform(X_test)\n",
    "#Definimos el algoritmo a utilizar\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "algoritmo = LogisticRegression()\n",
    "#Entrenamos al modelo\n",
    "algoritmo.fit(X_train, y_train)\n",
    "#Realizamos una prediccion\n",
    "y_pred = algoritmo.predict(X_test)\n",
    "#Verifico la matriz de confusion\n",
    "from sklearn.metrics import confusion_matrix\n",
    "matriz = confusion_matrix(y_test, y_pred)\n",
    "print('Matriz de Confusion: ')\n",
    "print(matriz)\n",
    "#Calculo la precision del modelo\n",
    "from sklearn.metrics import precision_score\n",
    "precision = precision_score(y_test, y_pred, average=\"binary\", pos_label=\"within an hour\")\n",
    "print('Precision del modelo: ', precision)\n",
    "#Calculamos la exactitud del modelo\n",
    "from sklearn.metrics import accuracy_score\n",
    "exactitud = accuracy_score(y_test, y_pred)\n",
    "print('Exactitud del modelo: ', exactitud)\n",
    "#Calculo la sensibilidad del modelo\n",
    "from sklearn.metrics import recall_score\n",
    "sensibilidad = recall_score(y_test, y_pred, average=\"binary\", pos_label=\"within an hour\")\n",
    "print(\"Sensibilidad del modelo: \", sensibilidad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Prediccion bedrooms**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Matriz de Confusion: \n",
      "[[5438  459]\n",
      " [1369  709]]\n",
      "Precision del modelo:  0.6070205479452054\n",
      "Exactitud del modelo:  0.7707836990595611\n",
      "Sensibilidad del modelo:  0.3411934552454283\n"
     ]
    }
   ],
   "source": [
    "#Declaramos las variables dependientes e idependientes para la regresion logistica\n",
    "Vars_Indep = df[['accommodates', 'bathrooms', 'price']] #Numericas\n",
    "Var_Dep = df['bedrooms'] #Dicotomica\n",
    "#Redefinimos las variables\n",
    "X = Vars_Indep\n",
    "y = Var_Dep\n",
    "#Dividimos el conjunto de datos en la parte de entrenamiento y prueba:\n",
    "X_train, X_test, y_train, y_test =train_test_split(X, y, test_size= 0.3, random_state=None)\n",
    "#Se escalan todos los datos\n",
    "escalar = StandardScaler()\n",
    "#Para realizar el escalamiento de las variables \"X\" tanto de entrenamiento como de prueba, utilizaremos fit_transform\n",
    "X_train = escalar.fit_transform(X_train)\n",
    "X_test = escalar.transform(X_test)\n",
    "#Definimos el algoritmo a utilizar\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "algoritmo = LogisticRegression()\n",
    "#Entrenamos al modelo\n",
    "algoritmo.fit(X_train, y_train)\n",
    "#Realizamos una prediccion\n",
    "y_pred = algoritmo.predict(X_test)\n",
    "#Verifico la matriz de confusion\n",
    "from sklearn.metrics import confusion_matrix\n",
    "matriz = confusion_matrix(y_test, y_pred)\n",
    "print('Matriz de Confusion: ')\n",
    "print(matriz)\n",
    "#Calculo la precision del modelo\n",
    "from sklearn.metrics import precision_score\n",
    "precision = precision_score(y_test, y_pred, average=\"binary\", pos_label=2)\n",
    "print('Precision del modelo: ', precision)\n",
    "#Calculamos la exactitud del modelo\n",
    "from sklearn.metrics import accuracy_score\n",
    "exactitud = accuracy_score(y_test, y_pred)\n",
    "print('Exactitud del modelo: ', exactitud)\n",
    "#Calculo la sensibilidad del modelo\n",
    "from sklearn.metrics import recall_score\n",
    "sensibilidad = recall_score(y_test, y_pred, average=\"binary\", pos_label=2)\n",
    "print(\"Sensibilidad del modelo: \", sensibilidad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Prediccion host_has_profile_pic**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Matriz de Confusion: \n",
      "[[   0  145]\n",
      " [   0 7830]]\n",
      "Precision del modelo:  0.9818181818181818\n",
      "Exactitud del modelo:  0.9818181818181818\n",
      "Sensibilidad del modelo:  1.0\n"
     ]
    }
   ],
   "source": [
    "#Declaramos las variables dependientes e idependientes para la regresion logistica\n",
    "Vars_Indep = df[['review_scores_communication', 'review_scores_value', 'reviews_per_month']] #Numericas\n",
    "Var_Dep = df['host_has_profile_pic'] #Dicotomica\n",
    "#Redefinimos las variables\n",
    "X = Vars_Indep\n",
    "y = Var_Dep\n",
    "#Dividimos el conjunto de datos en la parte de entrenamiento y prueba:\n",
    "X_train, X_test, y_train, y_test =train_test_split(X, y, test_size= 0.3, random_state=None)\n",
    "#Se escalan todos los datos\n",
    "escalar = StandardScaler()\n",
    "#Para realizar el escalamiento de las variables \"X\" tanto de entrenamiento como de prueba, utilizaremos fit_transform\n",
    "X_train = escalar.fit_transform(X_train)\n",
    "X_test = escalar.transform(X_test)\n",
    "#Definimos el algoritmo a utilizar\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "algoritmo = LogisticRegression()\n",
    "#Entrenamos al modelo\n",
    "algoritmo.fit(X_train, y_train)\n",
    "#Realizamos una prediccion\n",
    "y_pred = algoritmo.predict(X_test)\n",
    "#Verifico la matriz de confusion\n",
    "from sklearn.metrics import confusion_matrix\n",
    "matriz = confusion_matrix(y_test, y_pred)\n",
    "print('Matriz de Confusion: ')\n",
    "print(matriz)\n",
    "#Calculo la precision del modelo\n",
    "from sklearn.metrics import precision_score\n",
    "precision = precision_score(y_test, y_pred, average=\"binary\", pos_label=\"t\")\n",
    "print('Precision del modelo: ', precision)\n",
    "#Calculamos la exactitud del modelo\n",
    "from sklearn.metrics import accuracy_score\n",
    "exactitud = accuracy_score(y_test, y_pred)\n",
    "print('Exactitud del modelo: ', exactitud)\n",
    "#Calculo la sensibilidad del modelo\n",
    "from sklearn.metrics import recall_score\n",
    "sensibilidad = recall_score(y_test, y_pred, average=\"binary\", pos_label=\"t\")\n",
    "print(\"Sensibilidad del modelo: \", sensibilidad)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
